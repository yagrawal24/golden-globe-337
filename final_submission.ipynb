{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Final Notebook For the Golden GLobes Project - Project 1\n",
    "### Yash Agrawal, Sorie Yillah, Stephen Savas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first did data cleaning. This was deleteing data that were not ascii as this would help delete some langauges that had characters that wernt in the english langauge. We also deleted emoji's, links, and excess white space.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('gg2013.json')['text']\n",
    "\n",
    "# Define cleaning function\n",
    "def clean(text):\n",
    "    # Check for foreign language characters (alphabets beyond basic ASCII) not including emoji's since those tweets can be useful\n",
    "    if re.search(r'[^\\x00-\\x7F\\u263a-\\U0001f645]', text): \n",
    "        return None\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|pic.twitter\\S+', '', text)\n",
    "    \n",
    "    # Remove emojis (keep only non-emoji characters)\n",
    "    text = re.sub(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F]', '', text)\n",
    "    \n",
    "    # Remove excess whitespace\n",
    "    text = re.sub(r' +', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "df = df.apply(clean)\n",
    "cleaned_data = df.dropna()\n",
    "cleaned_data = cleaned_data[cleaned_data.str.strip() != \"\"]\n",
    "cleaned_data.to_csv('text_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next goal was to get winners mapped to award names. This meant getting predicted winners to predicted award names and then later mapping these predicted award names to the autograder award names to check our results.\n",
    "\n",
    "Our approach was:\n",
    "1) Filter tweets using the keyword \"wins\"\n",
    "2) Check these filtered tweet's left side for the subject's since the winner is likely to be the subject. Along with that, we aimed to get words that were near the subject so that if it was a long movie name or tv show name, we got a good portion of the name if not all of it. This subject approach also worked with people paired with spacy entity recognition of \"People\" entity's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the \"wins\" format data from cleaned data\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "win_keywords = r\"(\\bwins\\b)\"\n",
    "\n",
    "# List of award show names\n",
    "award_show_names = [\n",
    "    'GoldenGlobes', 'Golden Globes', 'Oscars', 'Academy Awards', 'Emmys',\n",
    "    'Grammy Awards', 'BAFTA', 'SAG Awards', 'Tony Awards', 'Cannes Film Festival',\n",
    "    'MTV Video Music Awards', 'American Music Awards', 'Critics Choice Awards',\n",
    "    \"People's Choice Awards\", 'Billboard Music Awards', 'BET Awards',\n",
    "    'Teen Choice Awards', 'Country Music Association Awards', 'Academy of Country Music Awards',\n",
    "    'Golden Globe Awards', 'Emmy Awards', 'Grammy', 'Cannes', 'MTV Awards',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions for extraction of winners/nominees/presenters and award names\n",
    "\n",
    "# Do this extraction if subj extraction fails\n",
    "def extract_entities_as_nominee(doc):\n",
    "    for ent in doc.ents:\n",
    "        # Consider entities such as PERSON, WORK_OF_ART, ORG, PRODUCT (e.g., \"Argo\")\n",
    "        if ent.label_ in ['PERSON', 'WORK_OF_ART', 'ORG', 'PRODUCT']:\n",
    "            return ent.text\n",
    "    return None\n",
    "\n",
    "# Do this extraction of winner first to get subj\n",
    "def extract_full_subject_as_nominee(doc):\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'nsubj' and token.head.text in ['wins', 'won', 'receives']:\n",
    "            subject_tokens = []\n",
    "            for left in token.lefts:\n",
    "                if left.dep_ in ['det', 'compound']:\n",
    "                    subject_tokens.append(left.text)\n",
    "            subject_tokens.append(token.text)\n",
    "            return ' '.join(subject_tokens)\n",
    "    return None\n",
    "\n",
    "# Extract the full award name starting with 'Best' using pattern matching and dependency parsing.\n",
    "# If we see punctuation or VERB we stop capturing since it marks the transition to another sentence part.\n",
    "def extract_award_name_after_best(doc):\n",
    "    award_phrases = []\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.text.lower() == 'best':\n",
    "            award_tokens = [token]\n",
    "            for j in range(i + 1, len(doc)):\n",
    "                next_token = doc[j]\n",
    "                if next_token.text in ('.', ',', ':', ';', '!', '?', '-', 'RT', '@', '#') or next_token.dep_ == 'punct':\n",
    "                    break\n",
    "                if next_token.pos_ in ('VERB', 'AUX') and next_token.dep_ in ('ROOT', 'conj'):\n",
    "                    break\n",
    "                if next_token.text.lower() == 'for':\n",
    "                    break\n",
    "                award_tokens.append(next_token)\n",
    "            award_phrase = ' '.join([t.text for t in award_tokens]).strip()\n",
    "            if award_phrase:\n",
    "                award_phrases.append(award_phrase)\n",
    "    if award_phrases:\n",
    "        return max(award_phrases, key=len)\n",
    "    return None\n",
    "\n",
    "# Extract the full award name preceding 'award' using pattern matching and dependency parsing.\n",
    "# If we see punctuation or VERB we stop capturing since it marks the transition to another sentence part.\n",
    "def extract_award_name_before_award(doc):\n",
    "    award_phrases = []\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.text.lower() == 'award':\n",
    "            award_tokens = []\n",
    "            for left_token in reversed(doc[:i]):\n",
    "                if left_token.text in ('.', ',', ':', ';', '!', '?', '-', 'RT', '@', '#') or left_token.dep_ == 'punct':\n",
    "                    break\n",
    "                if left_token.pos_ in ('VERB', 'AUX') and left_token.dep_ in ('ROOT', 'conj'):\n",
    "                    break\n",
    "                award_tokens.insert(0, left_token)\n",
    "            award_phrase = ' '.join([t.text for t in award_tokens]).strip()\n",
    "            if award_phrase:\n",
    "                award_phrases.append(award_phrase)\n",
    "    if award_phrases:\n",
    "        return max(award_phrases, key=len)\n",
    "    return None\n",
    "\n",
    "# Extract award name based on two styles: \"Best ....\" or \"... award\"\n",
    "def extract_award_names(text):\n",
    "    doc = nlp(text)\n",
    "    best_award = extract_award_name_after_best(doc)\n",
    "    award_name = extract_award_name_before_award(doc)\n",
    "    extracted_award = best_award or award_name\n",
    "    if extracted_award:\n",
    "        # Normalize award name for comparison\n",
    "        award_text = extracted_award.strip().lower()\n",
    "        award_show_names_lower = [name.lower() for name in award_show_names]\n",
    "        if award_text not in award_show_names_lower:\n",
    "            return extracted_award\n",
    "    return None\n",
    "\n",
    "# Many tweets are RT. Just delete the RT or @ symbol to make parsing and extraction easier.\n",
    "def ignore_rt_and_mentions(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = [token.text for token in doc if not (token.text.lower() == 'rt' or token.text.startswith('@'))]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "\n",
    "# Function to extract winner given a tweet in the format of \"X wins Y\"\n",
    "def find_award_winner(text):\n",
    "    \"\"\"Attempt to extract award information and return a structured output.\"\"\"\n",
    "    \n",
    "    # Ignore 'rt' and mentions but continue with the rest of the tweet\n",
    "    filtered_text = ignore_rt_and_mentions(text)\n",
    "    \n",
    "    doc = nlp(filtered_text)\n",
    "    \n",
    "    # Check if the tweet mentions winning or awards\n",
    "    if re.search(win_keywords, filtered_text, re.IGNORECASE):\n",
    "        # Extract the nominee (winner)\n",
    "        nominee = extract_full_subject_as_nominee(doc)\n",
    "        if not nominee:\n",
    "            nominee = extract_entities_as_nominee(doc)\n",
    "\n",
    "        # Extract the award category\n",
    "        award_category = extract_award_names(doc)\n",
    "        \n",
    "        if award_category != None and nominee != None:\n",
    "            return {award_category: nominee}\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/gg337/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m         d1\u001b[38;5;241m.\u001b[39mupdate(i)\n\u001b[1;32m     11\u001b[0m     d1\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwinners_and_awards.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mget_winners\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 8\u001b[0m, in \u001b[0;36mget_winners\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m win_output \u001b[38;5;241m=\u001b[39m win_data\u001b[38;5;241m.\u001b[39mapply(find_award_winner)\n\u001b[1;32m      7\u001b[0m win_output \u001b[38;5;241m=\u001b[39m win_output\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m----> 8\u001b[0m d1 \u001b[38;5;241m=\u001b[39m \u001b[43mwin_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m win_output:\n\u001b[1;32m     10\u001b[0m     d1\u001b[38;5;241m.\u001b[39mupdate(i)\n",
      "File \u001b[0;32m~/anaconda3/envs/gg337/lib/python3.10/site-packages/pandas/core/series.py:5391\u001b[0m, in \u001b[0;36mSeries.pop\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   5366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpop\u001b[39m(\u001b[38;5;28mself\u001b[39m, item: Hashable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   5367\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5368\u001b[0m \u001b[38;5;124;03m    Return item and drops from series. Raise KeyError if not found.\u001b[39;00m\n\u001b[1;32m   5369\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5389\u001b[0m \u001b[38;5;124;03m    dtype: int64\u001b[39;00m\n\u001b[1;32m   5390\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gg337/lib/python3.10/site-packages/pandas/core/generic.py:947\u001b[0m, in \u001b[0;36mNDFrame.pop\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpop\u001b[39m(\u001b[38;5;28mself\u001b[39m, item: Hashable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m Any:\n\u001b[0;32m--> 947\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m[item]\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/gg337/lib/python3.10/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/anaconda3/envs/gg337/lib/python3.10/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/anaconda3/envs/gg337/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "def get_winners():\n",
    "    cleaned_data = pd.read_csv('text_cleaned.csv')['text']\n",
    "    win_keywords = r\"(\\bwins\\b)\"\n",
    "    win_data = cleaned_data[cleaned_data.apply(lambda x: re.search(win_keywords, x) != None)]\n",
    "    win_data.to_csv(\"wins.csv\")\n",
    "    win_output = win_data.apply(find_award_winner)\n",
    "    win_output = win_output.dropna()\n",
    "    win_output.to_csv('winners_and_awards.csv')\n",
    "\n",
    "get_winners()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(FILL IN INFO FOR NOMINEES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next goal was to get presenters mapped to award names. This meant getting predicted presenters to predicted award names and then later mapping these predicted award names to the autograder award names to check our results.\n",
    "\n",
    "Our approach was:\n",
    "1) Filter tweets using the keywords: \"presenter|presenting|presented|presents|present'\"\n",
    "2) Similar to wins keywords, we filtern and check for a person entity existing (since presenter will always be a person) and then check for the existence of the word \"best\" or \"award\"\n",
    "3) Extract the person and award and store it similar to wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/yjt6cqd52wl4mfph_tj5l1cc0000gn/T/ipykernel_52963/3799592297.py:53: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  presenter_data = cleaned_df[cleaned_df['text'].str.contains(presenter_keywords, case=False, na=False)]\n"
     ]
    }
   ],
   "source": [
    "## Helper function for presenter extraction \n",
    "\n",
    "# Extract PERSON entities from text using spaCy, excluding award show names.\n",
    "def extract_person_entities(text):\n",
    "    doc = nlp(text)\n",
    "    persons = []\n",
    "    award_show_names_lower = [name.lower() for name in award_show_names]\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            # Normalize entity text for comparison\n",
    "            ent_text = ent.text.strip().lower()\n",
    "            if ent_text not in award_show_names_lower:\n",
    "                persons.append(ent.text)\n",
    "    return persons\n",
    "\n",
    "# Consolidate presenters per award in the tweet.\n",
    "def consolidate_presenters(row):\n",
    "    return row['Presenter_Award_Pairs']\n",
    "\n",
    "# Extract presenter and award pairs, excluding award show names.\n",
    "def extract_presenter_award_pairs(text):\n",
    "    doc = nlp(text)\n",
    "    people = extract_person_entities(text)\n",
    "    award_name = extract_award_names(text)\n",
    "    \n",
    "    if not award_name:\n",
    "        return {}\n",
    "    \n",
    "    award_presenters = {}\n",
    "    \n",
    "    award_show_names_lower = set(name.lower() for name in award_show_names)\n",
    "    \n",
    "    presenter_keywords = {'presenting', 'presented', 'presents', 'present'}\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        sentence_text = sent.text.lower()\n",
    "        if any(keyword in sentence_text for keyword in presenter_keywords):\n",
    "            for person in people:\n",
    "                person_lower = person.strip().lower()\n",
    "                if person_lower in award_show_names_lower:\n",
    "                    continue \n",
    "                if person_lower in sentence_text:\n",
    "                    award_presenters.setdefault(award_name, set()).add(person)\n",
    "    \n",
    "    # Convert sets to tuples\n",
    "    award_presenters = {k: tuple(v) for k, v in award_presenters.items()}\n",
    "    return award_presenters\n",
    "\n",
    "# Driver function to get presenter-award pairs.\n",
    "def process_presenter_data():\n",
    "    cleaned_df = pd.read_csv('text_cleaned.csv')\n",
    "    presenter_keywords = r'\\b(presenter|presenting|presented|presents|present)\\b'\n",
    "    presenter_data = cleaned_df[cleaned_df['text'].str.contains(presenter_keywords, case=False, na=False)]\n",
    "    presenter_data = presenter_data.reset_index(drop=True)\n",
    "    \n",
    "    # Apply entity extraction and pair extraction functions\n",
    "    presenter_data['Presenters'] = presenter_data['text'].apply(extract_person_entities)\n",
    "    presenter_data['Presenter_Award_Pairs'] = presenter_data['text'].apply(extract_presenter_award_pairs)\n",
    "    \n",
    "    # Keep only rows with non-empty Presenter_Award_Pairs\n",
    "    presenter_data = presenter_data[presenter_data['Presenter_Award_Pairs'].map(len) > 0]\n",
    "    \n",
    "    # Consolidate presenters per award\n",
    "    presenter_data['Consolidated_Pairs'] = presenter_data.apply(consolidate_presenters, axis=1)\n",
    "    \n",
    "    final_output = presenter_data['Consolidated_Pairs']\n",
    "    final_output.to_csv('presenter_award_consolidated.csv', header=False, index=False)\n",
    "\n",
    "\n",
    "process_presenter_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function to help map the winners and presenters stored in our csv files into the hardcoded dictionary for the autograder.\n",
    "It uses Cosine similarity to match our award names and the best similarity predicted award name to the actual award name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gg337",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
